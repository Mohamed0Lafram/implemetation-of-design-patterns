from Utils.utils import llm


# Initial reflection prompt for analyzing assistant responses
reflection_prompt = '''You are a prompt critique, and your task is to reflect on the generation history and provide feedback on how to
improve the generation process. You should analyze the content generated by the assistant and suggest improvements to generation.
prompt : '''


class Reflection_Pattern:
    """Class that runs an iterative loop of response generation and self-reflection to improve answer quality."""

    def __init__(self):
        self.chat_history = []

    def __generate_block(self, prompt, generation_history):
        """
        Generates a response using the LLM and appends it to the generation history.
        
        Args:
            prompt (str): The prompt to send to the model.
            generation_history (list): A list to store generation responses.
        """
        generation_response = llm(prompt)
        generation_history.append({
            "role": "assistant",
            "content": generation_response
        })

    def __reflection_block(self, generation_history, reflection_history):
        """
        Reflects on the generation history using the model and appends the reflection.
        
        Args:
            generation_history (list): List of generated assistant responses.
            reflection_history (list): List of reflections including the initial system prompt.
        """
        # Create reflection input from the current history
        complete_reflection_prompt = (
            f"{str(reflection_history[0]['content'])}.\n"
            f"reflection history: {str(reflection_history[1:])}.\n"
            f"generation history: {str(generation_history)}"
        )

        reflection_response = llm(complete_reflection_prompt)

        reflection_history.append({
            'role': 'assistant',
            'content': reflection_response
        })

    def run(self, query, iteration):
        """
        Runs the iterative generation-reflection process.
        
        Args:
            query (str): Initial user question.
            iteration (int): Number of iterations to run.

        Returns:
            dict: Includes final response, all intermediate responses, and reflections.
        """
        # Add initial user input to history
        self.chat_history.append({
            'role': 'user',
            'content': query
        })

        generation_history = []
        reflection_history = [
            {
                "role": 'system',
                'content': reflection_prompt
            }
        ]

        regeneration_prompt = {
            'role': 'system',
            'content': 'Regenerate the response based on the reflection.'
        }

        prompt = query

        # Run specified number of reflection-regen cycles
        for i in range(iteration):
            self.__generate_block(prompt, generation_history)
            self.__reflection_block(generation_history, reflection_history)

            # Prepare the next prompt incorporating reflection
            prompt = (
                f"{regeneration_prompt} reflection: {reflection_history[-1]['content']}. "
                f"the response: {generation_history[-1]['content']}"
            )

        # Final enhancement: Only output the improved answer
        prompt = (
            f"{regeneration_prompt} reflection: {reflection_history[-1]['content']}. "
            f"the response: {generation_history[-1]['content']}. "
            f"and do not mention anything except the answer to the question."
        )
        final_response = llm(prompt)

        self.chat_history.append({
            'role': 'assistant',
            'content': final_response
        })

        return {
            'iterations': iteration,
            'final_response': final_response,
            'all_responses': [[i, j['content']] for i, j in enumerate(generation_history)],
            'all_reflections': [[i, j['content']] for i, j in enumerate(reflection_history[1:])]
        }

